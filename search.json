[
  {
    "objectID": "health_data_analysis.html",
    "href": "health_data_analysis.html",
    "title": "Health Data Analysis",
    "section": "",
    "text": "Aim\nGiven series of data tables could I extract valuable information on patients an SQL database?\nLet’s give it a go!\nData \nData Tables \n\nAppendix A\nGender: 1 = Male, 2 = Female, 9 = Unknown\nEthnicity: 01 = White, 02 = Non-white, 03 = Unknown\nAppendix B\nPrimary diagnosis – ICD-10 code:\nProstate = C61, Breast = C50, Lung = C34, Colorectal = C18, C19, C20\nHow many patients were diagnosed with prostate cancer between 2015 and 2017?\nSELECT COUNT (Patient_table.patient_id) as number_of_patients \nFROM Patient_table \nJOIN Tumor_table \n ON Patient_table.patient_id = Tumor_table.patient_id \nJOIN Regimen_table \n ON Patient_table.patient_id = Regimen_table.patient_id \n WHERE primary_diagnosis = 'C61' and regimen_start_date \\&gt;= '01/01/2015' and regimen_start_date \\&lt;= '31/01/2017';\nCount the number of patients were diagnosed with prostate cancer who started a docetaxel regimen between 2015 and 2017?\nSELECT COUNT (Patient_table.patient_id) as number_of_patients\nFROM Patient_table\nJOIN Tumor_table\n  ON Patient_table.patient_id = Tumor_table.patient_id\nJOIN Regimen_table\n  ON Patient_table.patient_id = Regimen_table.patient_id\nWHERE primary_diagnosis = 'C61' \nand regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/01/2017' \nand regimen = 'docetaxel';\nStratified by ethnicity and age, count the number of patients diagnosed with breast cancer who started an eribulin regimen between 2015 and 2020. Only count women who were aged ≥50 years at the start of the regimen.\nSELECT COUNT(patient_id) AS number_of_patients, Ethnicity, Age\n  FROM (SELECT Patient_table.patient_id \n          , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, Regimen_table.regimen_start_date), Date_of_birth) &gt; Regimen_table.regimen_start_date \n         THEN datediff(year, Date_of_birth, Regimen_table.regimen_start_date) - 1 \n                         ELSE datediff(year, Date_of_birth, Regimen_table.regimen_start_date)\n               END) AS Age_at_regimen\n          , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, getdate()), Date_of_birth) &gt; getdate()\n                        THEN datediff(year, Date_of_birth, getdate()) - 1 \n                        ELSE datediff(year, Date_of_birth, getdate())\n               END) AS Age\n          , Regimen, primary_diagnosis, regimen_start_date, gender, Date_of_birth, Ethnicity \n          FROM Patient_table  \nJOIN Tumor_table\n  ON Patient_table.patient_id = Tumor_table.patient_id\nJOIN Regimen_table\n  ON Patient_table.patient_id = Regimen_table.patient_id\nWHERE primary_diagnosis = 'C50' -- breast cancer ICD-10 code\nand regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/12/2020' -- between 2015 and 2020\nand regimen = 'eribulin' \nand gender = 2 -- only women\n      ) AS results\n     WHERE results.Age_at_regimen &gt;= 50\nGROUP BY Ethnicity, Age;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tb011.github.io",
    "section": "",
    "text": "Welcome to my online website."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "List of projects:\nExample R code\nloadData &lt;- function(dataset_csv, list){\n  dataset &lt;- read.csv(dataset_csv)\n  results &lt;- lapply(dataset, list) \n  return(results)\n}\n\nresults &lt;- loadData(dataset_csv, list)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1"
  },
  {
    "objectID": "cross-validation.html",
    "href": "cross-validation.html",
    "title": "Cross-validation",
    "section": "",
    "text": "The question:\nWhen we do a 25% / 75% split, how do we know that the last quarter of the dataset (or a random selection of the dataset) is the best selection of the data to test on? Of course, it may randomly not be a good selection.\nBring in k-fold cross-validation."
  },
  {
    "objectID": "cross-validation.html#k-fold-cross-validation",
    "href": "cross-validation.html#k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nExample: 4-fold cross validation (k = 4)\n\nsplit the data in k-(4) folds.\nTrain on 3 partitions of the data, test on the remaining block.\n\nTake the following permutations:\nTrain {2, 3, 4} ; Test {1}:\n| 1 | - test\n| 2 | - train\n| 3 | - train\n| 4 | - train\nTrain {1, 3, 4} ; Test {2}:\n| 1 | - train\n| 2 | - test\n| 3 | - train\n| 4 | - train\nTrain {1, 2, 4} ; Test {3}:\n| 1 | - train\n| 2 | - train\n| 3 | - test\n| 4 | - train\nTrain {1, 2, 3} ; Test {4}:\n| 1 | - train\n| 2 | - train\n| 3 | - train\n| 4 | - test\nWe keep track of how well the machine learning algorithm does on teach test and then then take the average of the the test data scores.\nWe can then use this metric to compare its performance amongst other algorithms.\nK = 10, is a popular default. Meaning we train on 9 folds of the data, and test on the remaining unseen fold. In using k-fold cross-validation it means that every block of data is used for testing, rather than just 25%, for instance."
  },
  {
    "objectID": "cross-validation.html#leave-one-out-cross-validation",
    "href": "cross-validation.html#leave-one-out-cross-validation",
    "title": "Cross-validation",
    "section": "Leave one out cross-validation",
    "text": "Leave one out cross-validation\nWe could even take this example to the extreme and call each row (record/sample/ individual) as a “fold” and leave one individual out, and see how well the model is at classifying the remaining individual."
  },
  {
    "objectID": "cross-validation.html#stratified-k-fold-cross-validation",
    "href": "cross-validation.html#stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Stratified k-fold cross-validation",
    "text": "Stratified k-fold cross-validation\nThis is where the dilemma of unbalanced datasets come to bite. Let me give an example. In our typical health data set up, within a population the likelihood of observing a with a disease (i.e. COPD) is lower than that of observing an individual without COPD. The problem is exacerbated further with rare diseases. Therefore k-fold can fail to work, because we would like each fold to contain individuals with the disease. However, just by random chance, or if a disease is not very prevalent in a population our our sample of the population, then we may risk the event of not having cases (individuals with the disease) in one or many folds of our training data, which as we can envisage would have a poor performance when our model comes across a case in the first time in the test data!\nTherefore, we can use stratified k-fold cross validation to keep the same percentage of samples for each class, in each fold. For example, if our dataset has 10% of patients having COPD and 90% of patients not having COPD. Then by using StratifiedKFold the sci-kit learn library to keep the proportion of cases/non-cases (10% / 90%) the same in each of our k folds."
  },
  {
    "objectID": "cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "href": "cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Repeated stratified k-fold cross-validation",
    "text": "Repeated stratified k-fold cross-validation\nI came across a suggestion of using a repeated stratified k-fold cross-validation when I was training a logistic regression model on a diabetes dataset. I wondered if it could improve my models performance. And then everything else led from there….\nWhen using repeated stratified k-fold cross-validation we can repeat the random sampling of individuals in with a different randomisation in each repetition using RepeatedStratifiedKFold. This would mean that we would give our parameter k, for the number of folds and the parameter, n for the number of repeats. The result would be the average result across all folds from all runs. Please note that you are working on an balanced dataset, you can also use repeated k-fold cross-validation without the stratification!"
  },
  {
    "objectID": "cross-validation.html#final-comments",
    "href": "cross-validation.html#final-comments",
    "title": "Cross-validation",
    "section": "Final comments",
    "text": "Final comments\nIt can be much slower to partition the data in to k-folds (or repeated/stratified/repeated stratified k-folds) rather than splitting the data into a training and test set. Some times, depending on the application, computation time (in training and/or running the model) can be more important scoring a few decimal places higher on accuracy. So that is a trade off that something else for us to consider!"
  },
  {
    "objectID": "basics-jupyter.html",
    "href": "basics-jupyter.html",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "basics-jupyter.html#logistic-regression",
    "href": "basics-jupyter.html#logistic-regression",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  }
]