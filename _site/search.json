[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tb011.github.io",
    "section": "",
    "text": "Hello!\n\nAnd welcome to my blog. Here we discuss all things amongst my favourite things: health data, statistics, machine learning and evreything in between.\nLet’s go!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "List of projects:\nExample R code\nloadData &lt;- function(dataset_csv, list){\n  dataset &lt;- read.csv(dataset_csv)\n  results &lt;- lapply(dataset, list) \n  return(results)\n}\n\nresults &lt;- loadData(dataset_csv, list)"
  },
  {
    "objectID": "health_data_analysis.html",
    "href": "health_data_analysis.html",
    "title": "Health Data Analysis",
    "section": "",
    "text": "Aim\nGiven series of data tables could I extract valuable information on patients an SQL database?\nLet’s give it a go!\nData\nData Tables\n\nAppendix A\nGender: 1 = Male, 2 = Female, 9 = Unknown\nEthnicity: 01 = White, 02 = Non-white, 03 = Unknown\nAppendix B\nPrimary diagnosis – ICD-10 code:\nProstate = C61, Breast = C50, Lung = C34, Colorectal = C18, C19, C20\nHow many patients were diagnosed with prostate cancer between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients \n    FROM Patient_table \n    JOIN Tumor_table \n     ON Patient_table.patient_id = Tumor_table.patient_id \n    JOIN Regimen_table \n     ON Patient_table.patient_id = Regimen_table.patient_id \n     WHERE primary_diagnosis = 'C61' and regimen_start_date \\&gt;= '01/01/2015' and regimen_start_date \\&lt;= '31/01/2017';\nCount the number of patients were diagnosed with prostate cancer who started a docetaxel regimen between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients\n    FROM Patient_table\n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C61' \n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/01/2017' \n    and regimen = 'docetaxel';\nStratified by ethnicity and age, count the number of patients diagnosed with breast cancer who started an eribulin regimen between 2015 and 2020. Only count women who were aged ≥50 years at the start of the regimen.\n\n\n    SELECT COUNT(patient_id) AS number_of_patients, Ethnicity, Age\n      FROM (SELECT Patient_table.patient_id \n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, Regimen_table.regimen_start_date), Date_of_birth) &gt; Regimen_table.regimen_start_date \n             THEN datediff(year, Date_of_birth, Regimen_table.regimen_start_date) - 1 \n                             ELSE datediff(year, Date_of_birth, Regimen_table.regimen_start_date)\n                   END) AS Age_at_regimen\n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, getdate()), Date_of_birth) &gt; getdate()\n                            THEN datediff(year, Date_of_birth, getdate()) - 1 \n                            ELSE datediff(year, Date_of_birth, getdate())\n                   END) AS Age\n              , Regimen, primary_diagnosis, regimen_start_date, gender, Date_of_birth, Ethnicity \n              FROM Patient_table  \n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C50' -- breast cancer ICD-10 code\n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/12/2020' -- between 2015 and 2020\n    and regimen = 'eribulin' \n    and gender = 2 -- only women\n          ) AS results\n         WHERE results.Age_at_regimen &gt;= 50\n    GROUP BY Ethnicity, Age;"
  },
  {
    "objectID": "health_data_analysis.html#data-tables",
    "href": "health_data_analysis.html#data-tables",
    "title": "Health Data Analysis",
    "section": "Data Tables",
    "text": "Data Tables\n\n\nAPPENDIX A\nGender: 1 = Male, 2 = Female, 9 = Unknown\nEthnicity: 01 = White, 02 = Non-white, 03 = Unknown\n\n\nAPPENDIX B\nPrimary diagnosis – ICD-10 code:\nProstate = C61, Breast = C50, Lung = C34, Colorectal = C18, C19, C20"
  },
  {
    "objectID": "health_data_analysis.html#how-many-patients-were-diagnosed-with-prostate-cancer-between-2015-and-2017",
    "href": "health_data_analysis.html#how-many-patients-were-diagnosed-with-prostate-cancer-between-2015-and-2017",
    "title": "Health Data Analysis",
    "section": "How many patients were diagnosed with prostate cancer between 2015 and 2017?",
    "text": "How many patients were diagnosed with prostate cancer between 2015 and 2017?\n\nSELECT COUNT (Patient_table.patient_id) as number_of_patients \nFROM Patient_table \nJOIN Tumor_table \n ON Patient_table.patient_id = Tumor_table.patient_id \nJOIN Regimen_table \n ON Patient_table.patient_id = Regimen_table.patient_id \n WHERE primary_diagnosis = 'C61' and regimen_start_date \\&gt;= '01/01/2015' and regimen_start_date \\&lt;= '31/01/2017';"
  },
  {
    "objectID": "k-fold-validation.html",
    "href": "k-fold-validation.html",
    "title": "Health Data Analysis",
    "section": "",
    "text": "Aim\nWhen we do a 25%/75% split. How do we know that the i.e. last quarter of the dataset is the best section of the data to test on? Or the random selection we made.\nIt may randomly not be a good selection.\nBring in k-fold cross-validation.\nExample: 4-fold cross validation (k = 4)\n1. split the data in k- (4) folds.\n2. Train on 3 partitions of the data, test on the remaining block. Take the below permutations\nTrain {2, 3, 4} ; Test {1}\n| 1 | - test\n| 2 | - train\n| 3 | - train\n| 4 | - train\nTrain {1, 3, 4} ; Test {2}\n| 1 | - train\n| 2 | - test\n| 3 | - train\n| 4 | - train\nTrain {1, 2, 4} ; Test {3}\n| 1 | - train\n| 2 | - train\n| 3 | - test\n| 4 | - train\nTrain {1, 2, 3} ; Test {4}\n| 1 | - train\n| 2 | - train\n| 3 | - train\n| 4 | - test"
  },
  {
    "objectID": "cross-validation.html",
    "href": "cross-validation.html",
    "title": "Cross-validation",
    "section": "",
    "text": "The question:\nWhen we do a 25% / 75% split, how do we know that the last quarter of the dataset (or a random selection of the dataset) is the best selection of the data to test on? Of course, it may randomly not be a good selection.\nBring in k-fold cross-validation."
  },
  {
    "objectID": "cross-validation.html#k-fold-cross-validation",
    "href": "cross-validation.html#k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nExample: 4-fold cross validation (k = 4)\n\nsplit the data in k-(4) folds.\nTrain on 3 partitions of the data, test on the remaining block.\n\nTake the following permutations:\nTrain {2, 3, 4} ; Test {1}:\n| 1 | - test\n| 2 | - train\n| 3 | - train\n| 4 | - train\nTrain {1, 3, 4} ; Test {2}:\n| 1 | - train\n| 2 | - test\n| 3 | - train\n| 4 | - train\nTrain {1, 2, 4} ; Test {3}:\n| 1 | - train\n| 2 | - train\n| 3 | - test\n| 4 | - train\nTrain {1, 2, 3} ; Test {4}:\n| 1 | - train\n| 2 | - train\n| 3 | - train\n| 4 | - test\nWe keep track of how well the machine learning algorithm does on teach test and then then take the average of the the test data scores.\nWe can then use this metric to compare its performance amongst other algorithms.\nK = 10, is a popular default. Meaning we train on 9 folds of the data, and test on the remaining unseen fold. In using k-fold cross-validation it means that every block of data is used for testing, rather than just 25%, for instance."
  },
  {
    "objectID": "cross-validation.html#leave-one-out-cross-validation",
    "href": "cross-validation.html#leave-one-out-cross-validation",
    "title": "Cross-validation",
    "section": "Leave one out cross-validation",
    "text": "Leave one out cross-validation\nWe could even take this example to the extreme and call each row (record/sample/ individual) as a “fold” and leave one individual out, and see how well the model is at classifying the remaining individual."
  },
  {
    "objectID": "cross-validation.html#final-comments",
    "href": "cross-validation.html#final-comments",
    "title": "Cross-validation",
    "section": "Final comments",
    "text": "Final comments\nIt can be much slower to partition the data in to k-folds (or repeated/stratified/repeated stratified k-folds) rather than splitting the data into a training and test set. Some times, depending on the application, computation time (in training and/or running the model) can be more important scoring a few decimal places higher on accuracy. So that is a trade off that something else for us to consider!"
  },
  {
    "objectID": "cross-validation.html#resources",
    "href": "cross-validation.html#resources",
    "title": "Cross-validation",
    "section": "Resources",
    "text": "Resources\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
  },
  {
    "objectID": "cross-validation.html#stratified-k-fold-cross-validation",
    "href": "cross-validation.html#stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Stratified k-fold cross-validation",
    "text": "Stratified k-fold cross-validation\nThis is where the dilemma of unbalanced datasets come to bite. Let me give an example. In our typical health data set up, within a population the likelihood of observing a with a disease (i.e. COPD) is lower than that of observing an individual without COPD. The problem is exacerbated further with rare diseases. Therefore k-fold can fail to work, because we would like each fold to contain individuals with the disease. However, just by random chance, or if a disease is not very prevalent in a population our our sample of the population, then we may risk the event of not having cases (individuals with the disease) in one or many folds of our training data, which as we can envisage would have a poor performance when our model comes across a case in the first time in the test data!\nTherefore, we can use stratified k-fold cross validation to keep the same percentage of samples for each class, in each fold. For example, if our dataset has 10% of patients having COPD and 90% of patients not having COPD. Then by using StratifiedKFold the sci-kit learn library to keep the proportion of cases/non-cases (10% / 90%) the same in each of our k folds."
  },
  {
    "objectID": "cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "href": "cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Repeated stratified k-fold cross-validation",
    "text": "Repeated stratified k-fold cross-validation\nI came across a suggestion of using a repeated stratified k-fold cross-validation when I was training a logistic regression model on a diabetes dataset. I wondered if it could improve my models performance. And then everything else led from there….\nWhen using repeated stratified k-fold cross-validation we can repeat the random sampling of individuals in with a different randomisation in each repetition using RepeatedStratifiedKFold. This would mean that we would give our parameter k, for the number of folds and the parameter, n for the number of repeats. The result would be the average result across all folds from all runs. Please note that you are working on an balanced dataset, you can also use repeated k-fold cross-validation without the stratification!"
  },
  {
    "objectID": "cross-validation.html#references",
    "href": "cross-validation.html#references",
    "title": "Cross-validation",
    "section": "References",
    "text": "References\nhttps://scikit-learn.org/stable/index.html\nhttps://machinelearningmastery.com/"
  },
  {
    "objectID": "basics-jupyter.html",
    "href": "basics-jupyter.html",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "basics-jupyter.html#polar-axis",
    "href": "basics-jupyter.html#polar-axis",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\nCode\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\n\n\nCode\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "functional_cdc_diabetes.html",
    "href": "functional_cdc_diabetes.html",
    "title": "tb011.github.io",
    "section": "",
    "text": "# Using Logistic regression as a classifier (Machine Learning) \n\n\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\n# The data\n\n# 253,680 individuals, with 22 features\n# This dataset was released by the CDC, Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services\n# https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\n\n# The model - Logistic regression (Multinomal model) \n\n# Logistic regression is typically not a classifier however; when using sci-kit learn:\n# \"The numerical output of the logistic regression, which is the predicted probability, can be used as a classifier by applying a threshold (by default 0.5) to it. \n# This is how it is implemented in scikit-learn, so it expects a categorical target, making the Logistic Regression a classifier.\"\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "basics-jupyter.html#logistic-regression",
    "href": "basics-jupyter.html#logistic-regression",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = 'data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/basics-jupyter.html",
    "href": "posts/basics-jupyter.html",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/basics-jupyter.html#logistic-regression",
    "href": "posts/basics-jupyter.html#logistic-regression",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/cross-validation.html",
    "href": "posts/cross-validation.html",
    "title": "Cross-validation",
    "section": "",
    "text": "The question:\nWhen we do a 25% / 75% split, how do we know that the last quarter of the dataset (or a random selection of the dataset) is the best selection of the data to test on? Of course, it may randomly not be a good selection.\nBring in k-fold cross-validation."
  },
  {
    "objectID": "posts/cross-validation.html#k-fold-cross-validation",
    "href": "posts/cross-validation.html#k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nExample: 4-fold cross validation (k = 4)\n\nsplit the data in k-(4) folds.\nTrain on 3 partitions of the data, test on the remaining block.\n\nTake the following permutations:\nTrain {2, 3, 4} ; Test {1}:\n| 1 | - test\n| 2 | - train\n| 3 | - train\n| 4 | - train\nTrain {1, 3, 4} ; Test {2}:\n| 1 | - train\n| 2 | - test\n| 3 | - train\n| 4 | - train\nTrain {1, 2, 4} ; Test {3}:\n| 1 | - train\n| 2 | - train\n| 3 | - test\n| 4 | - train\nTrain {1, 2, 3} ; Test {4}:\n| 1 | - train\n| 2 | - train\n| 3 | - train\n| 4 | - test\nWe keep track of how well the machine learning algorithm does on teach test and then then take the average of the the test data scores.\nWe can then use this metric to compare its performance amongst other algorithms.\nK = 10, is a popular default. Meaning we train on 9 folds of the data, and test on the remaining unseen fold. In using k-fold cross-validation it means that every block of data is used for testing, rather than just 25%, for instance."
  },
  {
    "objectID": "posts/cross-validation.html#leave-one-out-cross-validation",
    "href": "posts/cross-validation.html#leave-one-out-cross-validation",
    "title": "Cross-validation",
    "section": "Leave one out cross-validation",
    "text": "Leave one out cross-validation\nWe could even take this example to the extreme and call each row (record/sample/ individual) as a “fold” and leave one individual out, and see how well the model is at classifying the remaining individual."
  },
  {
    "objectID": "posts/cross-validation.html#stratified-k-fold-cross-validation",
    "href": "posts/cross-validation.html#stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Stratified k-fold cross-validation",
    "text": "Stratified k-fold cross-validation\nThis is where the dilemma of unbalanced datasets come to bite. Let me give an example. In our typical health data set up, within a population the likelihood of observing a with a disease (i.e. COPD) is lower than that of observing an individual without COPD. The problem is exacerbated further with rare diseases. Therefore k-fold can fail to work, because we would like each fold to contain individuals with the disease. However, just by random chance, or if a disease is not very prevalent in a population our our sample of the population, then we may risk the event of not having cases (individuals with the disease) in one or many folds of our training data, which as we can envisage would have a poor performance when our model comes across a case in the first time in the test data!\nTherefore, we can use stratified k-fold cross validation to keep the same percentage of samples for each class, in each fold. For example, if our dataset has 10% of patients having COPD and 90% of patients not having COPD. Then by using StratifiedKFold the sci-kit learn library to keep the proportion of cases/non-cases (10% / 90%) the same in each of our k folds."
  },
  {
    "objectID": "posts/cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "href": "posts/cross-validation.html#repeated-stratified-k-fold-cross-validation",
    "title": "Cross-validation",
    "section": "Repeated stratified k-fold cross-validation",
    "text": "Repeated stratified k-fold cross-validation\nI came across a suggestion of using a repeated stratified k-fold cross-validation when I was training a logistic regression model on a diabetes dataset. I wondered if it could improve my models performance. And then everything else led from there….\nWhen using repeated stratified k-fold cross-validation we can repeat the random sampling of individuals in with a different randomisation in each repetition using RepeatedStratifiedKFold. This would mean that we would give our parameter k, for the number of folds and the parameter, n for the number of repeats. The result would be the average result across all folds from all runs. Please note that you are working on an balanced dataset, you can also use repeated k-fold cross-validation without the stratification!"
  },
  {
    "objectID": "posts/cross-validation.html#final-comments",
    "href": "posts/cross-validation.html#final-comments",
    "title": "Cross-validation",
    "section": "Final comments",
    "text": "Final comments\nIt can be much slower to partition the data in to k-folds (or repeated/stratified/repeated stratified k-folds) rather than splitting the data into a training and test set. Some times, depending on the application, computation time (in training and/or running the model) can be more important scoring a few decimal places higher on accuracy. So that is a trade off that something else for us to consider!"
  },
  {
    "objectID": "posts/health_data_analysis.html",
    "href": "posts/health_data_analysis.html",
    "title": "Health Data Analysis",
    "section": "",
    "text": "Aim\nGiven series of data tables could I extract valuable information on patients an SQL database?\nLet’s give it a go!\nData\nData Tables\n\nAppendix A\nGender: 1 = Male, 2 = Female, 9 = Unknown\nEthnicity: 01 = White, 02 = Non-white, 03 = Unknown\nAppendix B\nPrimary diagnosis – ICD-10 code:\nProstate = C61, Breast = C50, Lung = C34, Colorectal = C18, C19, C20\nHow many patients were diagnosed with prostate cancer between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients \n    FROM Patient_table \n    JOIN Tumor_table \n     ON Patient_table.patient_id = Tumor_table.patient_id \n    JOIN Regimen_table \n     ON Patient_table.patient_id = Regimen_table.patient_id \n     WHERE primary_diagnosis = 'C61' and regimen_start_date \\&gt;= '01/01/2015' and regimen_start_date \\&lt;= '31/01/2017';\nCount the number of patients were diagnosed with prostate cancer who started a docetaxel regimen between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients\n    FROM Patient_table\n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C61' \n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/01/2017' \n    and regimen = 'docetaxel';\nStratified by ethnicity and age, count the number of patients diagnosed with breast cancer who started an eribulin regimen between 2015 and 2020. Only count women who were aged ≥50 years at the start of the regimen.\n\n\n    SELECT COUNT(patient_id) AS number_of_patients, Ethnicity, Age\n      FROM (SELECT Patient_table.patient_id \n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, Regimen_table.regimen_start_date), Date_of_birth) &gt; Regimen_table.regimen_start_date \n             THEN datediff(year, Date_of_birth, Regimen_table.regimen_start_date) - 1 \n                             ELSE datediff(year, Date_of_birth, Regimen_table.regimen_start_date)\n                   END) AS Age_at_regimen\n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, getdate()), Date_of_birth) &gt; getdate()\n                            THEN datediff(year, Date_of_birth, getdate()) - 1 \n                            ELSE datediff(year, Date_of_birth, getdate())\n                   END) AS Age\n              , Regimen, primary_diagnosis, regimen_start_date, gender, Date_of_birth, Ethnicity \n              FROM Patient_table  \n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C50' -- breast cancer ICD-10 code\n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/12/2020' -- between 2015 and 2020\n    and regimen = 'eribulin' \n    and gender = 2 -- only women\n          ) AS results\n         WHERE results.Age_at_regimen &gt;= 50\n    GROUP BY Ethnicity, Age;"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nCross-validation\n\n\nMachine learning, Cross-validation, Python\n\n\n\n\nHealth Data Analysis\n\n\nSQL, Health Data, Database\n\n\n\n\nLogistic Regression (classification)\n\n\nMachine Learning, Health data, Python\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/logistic-regression.html",
    "href": "posts/logistic-regression.html",
    "title": "Logistic Regression (classification)",
    "section": "",
    "text": "It turns out this this topic is a little bit controversial for some. Some say that logistic regression is a regression model (hence the name), other swear it is a classification model. Well… I’m here to say that both sides are right! Traditionally, logistic regression is a statistical regression model as it gives us the probability of an observation belonging to class. However, if this method is coupled with a ‘decider/threshold’ then it can indeed be turned into a classification method. For instance, lets say our model states that given the data, a specific patient has the estimated probability of 0.55 of belonging to the diabetics class (p = 0.55). We can apply a threshold at 0.5 which classifies patients below this threshold to belong to the non-diabetes class, and those &gt;= 0.5 to belong to the diabetes class, and to be therefore classified as a diabetic. Sci-kit learn library uses 0.5 as a threshold, but there’s nothing to stop you from defining your own value! Let’s take a walk through an example together."
  },
  {
    "objectID": "posts/logistic-regression.html#logistic-regression",
    "href": "posts/logistic-regression.html#logistic-regression",
    "title": "Diabetes Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n# Exploratory data analysis \n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/sql-health-data.html",
    "href": "posts/sql-health-data.html",
    "title": "Health Data Analysis",
    "section": "",
    "text": "Aim\nGiven series of data tables could I extract valuable information on patients an SQL database?\nLet’s give it a go!\nData\nData Tables\n\nAppendix A\nGender: 1 = Male, 2 = Female, 9 = Unknown\nEthnicity: 01 = White, 02 = Non-white, 03 = Unknown\nAppendix B\nPrimary diagnosis – ICD-10 code:\nProstate = C61, Breast = C50, Lung = C34, Colorectal = C18, C19, C20\nHow many patients were diagnosed with prostate cancer between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients \n    FROM Patient_table \n    JOIN Tumor_table \n     ON Patient_table.patient_id = Tumor_table.patient_id \n    JOIN Regimen_table \n     ON Patient_table.patient_id = Regimen_table.patient_id \n     WHERE primary_diagnosis = 'C61' and regimen_start_date \\&gt;= '01/01/2015' and regimen_start_date \\&lt;= '31/01/2017';\nCount the number of patients were diagnosed with prostate cancer who started a docetaxel regimen between 2015 and 2017?\n    SELECT COUNT (Patient_table.patient_id) as number_of_patients\n    FROM Patient_table\n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C61' \n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/01/2017' \n    and regimen = 'docetaxel';\nStratified by ethnicity and age, count the number of patients diagnosed with breast cancer who started an eribulin regimen between 2015 and 2020. Only count women who were aged ≥50 years at the start of the regimen.\n\n\n    SELECT COUNT(patient_id) AS number_of_patients, Ethnicity, Age\n      FROM (SELECT Patient_table.patient_id \n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, Regimen_table.regimen_start_date), Date_of_birth) &gt; Regimen_table.regimen_start_date \n             THEN datediff(year, Date_of_birth, Regimen_table.regimen_start_date) - 1 \n                             ELSE datediff(year, Date_of_birth, Regimen_table.regimen_start_date)\n                   END) AS Age_at_regimen\n              , (CASE WHEN dateadd(year, datediff (year, Date_of_birth, getdate()), Date_of_birth) &gt; getdate()\n                            THEN datediff(year, Date_of_birth, getdate()) - 1 \n                            ELSE datediff(year, Date_of_birth, getdate())\n                   END) AS Age\n              , Regimen, primary_diagnosis, regimen_start_date, gender, Date_of_birth, Ethnicity \n              FROM Patient_table  \n    JOIN Tumor_table\n      ON Patient_table.patient_id = Tumor_table.patient_id\n    JOIN Regimen_table\n      ON Patient_table.patient_id = Regimen_table.patient_id\n    WHERE primary_diagnosis = 'C50' -- breast cancer ICD-10 code\n    and regimen_start_date &gt;= '01/01/2015' and  regimen_start_date &lt;= '31/12/2020' -- between 2015 and 2020\n    and regimen = 'eribulin' \n    and gender = 2 -- only women\n          ) AS results\n         WHERE results.Age_at_regimen &gt;= 50\n    GROUP BY Ethnicity, Age;"
  },
  {
    "objectID": "posts/logistic-regression.html#logistic-regression-bionomial",
    "href": "posts/logistic-regression.html#logistic-regression-bionomial",
    "title": "Logistic Regression",
    "section": "",
    "text": "Using Logistic regression as a classifier (Machine Learning)\n\n# These are all of the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\n# Let's start with some exploratory data analysis (EDA)\n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/logistic-regression.html#logistic-regression-as-a-classifier",
    "href": "posts/logistic-regression.html#logistic-regression-as-a-classifier",
    "title": "Logistic Regression (classifer)",
    "section": "",
    "text": "It turns out this this topic is a little bit controversial for some. Some say that logistic regression is a regression model (hence the name), other swear it is a classification model. Well… I’m here to say that both sides are right! Traditionally, logistic regression is a statistical regression model as it gives us the probability of an observation belonging to class. However, if this method is coupled with a ‘decider/threshold’ then it can indeed be turned into a classification method. For instance, lets say our model states that given the data, a specific patient has the estimated probability of 0.55 of belonging to the diabetics class (p = 0.55). We can apply a threshold at 0.5 which classifies patients below this threshold to belong to the non-diabetes class, and those &gt;= 0.5 to belong to the diabetes class, and to be therefore classified as a diabetic. Sci-kit learn library uses 0.5 as a threshold, but there’s nothing to stop you from defining your own value! So, let’s take a walk through an example together.. first we will take a look at a binomial example and then, approach a multinomial logistic regression with a three classes.\n\n\n\n\n\n# These are the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter"
  },
  {
    "objectID": "posts/logistic-regression.html#lets-program-with-python",
    "href": "posts/logistic-regression.html#lets-program-with-python",
    "title": "Logistic Regression",
    "section": "Let’s program with Python",
    "text": "Let’s program with Python\n\n# These are all of the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\n# Let's start with some exploratory data analysis (EDA)\n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/logistic-regression.html#program-with-python",
    "href": "posts/logistic-regression.html#program-with-python",
    "title": "Logistic Regression",
    "section": "Program with Python",
    "text": "Program with Python\n\n# These are all of the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\n# Let's start with some exploratory data analysis (EDA)\n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    #y = df.loc[:, df.columns == y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/logistic-regression.html#lets-program-this-in-python",
    "href": "posts/logistic-regression.html#lets-program-this-in-python",
    "title": "Logistic Regression",
    "section": "Let’s program this in python",
    "text": "Let’s program this in python\n\n# These are the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter"
  },
  {
    "objectID": "posts/logistic-regression.html#exploratory-data-anaysis-eda",
    "href": "posts/logistic-regression.html#exploratory-data-anaysis-eda",
    "title": "Logistic Regression (classification)",
    "section": "Exploratory Data Anaysis (EDA)",
    "text": "Exploratory Data Anaysis (EDA)\n\n# Let's create a function that will give us some useful starting statistics about our dataset\n\ndef eda (df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\n\ndef prepData (data_file, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model, \n                  random_state,\n                  max_iter):\n    \n    model = LogisticRegression(random_state = random_state, max_iter = max_iter)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred, model\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\n\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\nhead, shape, missing_values = eda(df)\ndisplay(head)\ndisplay(shape)\ndisplay(missing_values)\ndf, X, y = prepData(data_file = '../data/diabetes_binary_health_indicators_BRFSS2015.csv',\n                        y_label_name = 'Diabetes_binary')\ndf\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n(253680, 22)\n\n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n253675\n0.0\n1.0\n1.0\n1.0\n45.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n5.0\n0.0\n1.0\n5.0\n6.0\n7.0\n\n\n253676\n1.0\n1.0\n1.0\n1.0\n18.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n4.0\n0.0\n0.0\n1.0\n0.0\n11.0\n2.0\n4.0\n\n\n253677\n0.0\n0.0\n0.0\n1.0\n28.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n2.0\n5.0\n2.0\n\n\n253678\n0.0\n1.0\n0.0\n1.0\n23.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n3.0\n0.0\n0.0\n0.0\n1.0\n7.0\n5.0\n1.0\n\n\n253679\n1.0\n1.0\n1.0\n1.0\n25.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n9.0\n6.0\n2.0\n\n\n\n\n253680 rows × 22 columns"
  },
  {
    "objectID": "posts/logistic-regression.html#bionomial-logistic-regression",
    "href": "posts/logistic-regression.html#bionomial-logistic-regression",
    "title": "Logistic Regression (classification)",
    "section": "Bionomial logistic regression",
    "text": "Bionomial logistic regression\nFor this dataset we are going to use the CDC Diabetes Health Indicators dataset which I came across on the UCI Irvine Machine Learning Repository. We are also going to program this in Python, due to the libraries we can utilise with it.\n\n# These are the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter\n\n\nExploratory Data Analysis (EDA)\n\n# Let's load in our data\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\n\n\n# Create a function that will give us some useful starting statistics about our dataset\n\ndef eda(df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\nhead, shape, missing_values = eda(df)\n\n\nhead\n# We can see our label, y named 'Diabetes_binary' is the first feature in our dataset \n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nshape\n# We have 253,680 individuals and 22 features in total\n\n(253680, 22)\n\n\n\nmissing_values\n# No missing values have been observed, which means no need to drop or impute any values \n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n# Let's seperate the data into the feature (independent) variables and the target variable (label) \n\ndef prepData (df, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n  \ndf, X, y = prepData(df, y_label_name = 'Diabetes_binary')\n \n\ny\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n253675    0.0\n253676    1.0\n253677    0.0\n253678    0.0\n253679    1.0\nName: Diabetes_binary, Length: 253680, dtype: float64"
  },
  {
    "objectID": "posts/logistic-regression.html#references",
    "href": "posts/logistic-regression.html#references",
    "title": "Logistic Regression (classification)",
    "section": "References",
    "text": "References\n\nUCI: (https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators)\nThe Hundred Page Machine Learning Book, by Andriy Burkov: https://github.com/lyric12345/The-Hundred-Page-Machine-Learning-Book-by-Andriy-Burkov\nRemember precision and recall forever: https://www.youtube.com/watch?v=qWfzIYCvBqo"
  },
  {
    "objectID": "posts/logistic-regression.html#mul",
    "href": "posts/logistic-regression.html#mul",
    "title": "Logistic Regression (classification)",
    "section": "Mul",
    "text": "Mul"
  },
  {
    "objectID": "posts/logistic-regression.html#multinomial-logistic-regression",
    "href": "posts/logistic-regression.html#multinomial-logistic-regression",
    "title": "Logistic Regression (classification)",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression"
  },
  {
    "objectID": "posts/logistic-regression.html#defining-a-training-and-test-set",
    "href": "posts/logistic-regression.html#defining-a-training-and-test-set",
    "title": "Logistic Regression (classification)",
    "section": "Defining a training and test set",
    "text": "Defining a training and test set\nWe are going to the use the train_test_split function from the sci-kit learn library to divide the dataset in a training/test split (75% and 25%, respectively).\nWe will set the seed arbitrarily to ‘16’ so that we can reproduce this split each time we run the code.\nThis will result in four arrays: X_train, y_train, X_test, y_test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 16)"
  },
  {
    "objectID": "posts/logistic-regression.html#applying-a-logistic-regression-model",
    "href": "posts/logistic-regression.html#applying-a-logistic-regression-model",
    "title": "Logistic Regression (classification)",
    "section": "Applying a logistic regression model",
    "text": "Applying a logistic regression model\n\n# Let's define a function that applies our choosen model to training and test data\ndef create_model (X_train,\n                  y_train,\n                  X_test,\n                  model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return y_pred\n\n# Instantiate the logistic regression model with enough iterations for convergence\n#model = LogisticRegression(random_state = 16, max_iter = 500)\nmodel =  LogisticRegression(random_state = 16, max_iter = 5000, class_weight = 'balanced')\n\n# now let's run the function\ny_pred = create_model(X_train, y_train,  X_test, model = model)\n\n# Here are our predicted values for y! \nCounter(y_pred) # Class 1: 60851; Class 2: 2569\n\nCounter({np.float64(0.0): 41516, np.float64(1.0): 21904})"
  },
  {
    "objectID": "posts/logistic-regression.html#model-performance",
    "href": "posts/logistic-regression.html#model-performance",
    "title": "Logistic Regression (classification)",
    "section": "Model performance",
    "text": "Model performance\n\nConfusion matrix\nLet’s visualise how well our model was able to predict the class labels using a confusion matrix\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix\n\narray([[39532, 15194],\n       [ 1984,  6710]])\n\n\nLet’s create a nice visual heatmap\n\ndef makeHeatmap (class_names, cnf_matrix): \n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap= \"Oranges\" ,fmt='g')\n    ax.xaxis.set_label_position(\"bottom\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label');\n    \n    \nmakeHeatmap(class_names = [0,1], cnf_matrix = cnf_matrix)\n\n\n\n\n\n\n\n\n\ndef calcPresRecall (y_test, y_pred, target_names): \n    report = classification_report(y_test, y_pred, target_names = target_names)\n    return report\n  \nreport = calcPresRecall (target_names = ['without diabetes', 'with diabetes'], \n         y_test = y_test, \n         y_pred = y_pred)\n\nprint(report)\n\n                  precision    recall  f1-score   support\n\nwithout diabetes       0.95      0.72      0.82     54726\n   with diabetes       0.31      0.77      0.44      8694\n\n        accuracy                           0.73     63420\n       macro avg       0.63      0.75      0.63     63420\n    weighted avg       0.86      0.73      0.77     63420\n\n\n\n\n\nAccuracy\nFirst of all a note on accuracy: accuracy can be described as the correction predictions out of the total number of observations (taking both/all classes into account) (TP + TN / TP + FP + TN + FN). So in this case: how many actual diabetics did classify as diabetic and how many non-diabetics did we classify as non-diabetic?\nTypically, a classification rate of 80% is regarded as “good”. However, the accuracy metric has a limitation; it doesn’t capture the false positives, or false negatives for that matter. In other words, how many observations did you misclassify? This is why we are going to look at precison and recall.\n\n\nPrecision\nPrecision = out of all people that were classified as diabetic, the percentage who were truly diabetic (TP / TP + FP). When this logistic regression model predicts patients are going to suffer from diabetes, that patients actually have diabetes 81% of the time.\n\n\nRecall\nRecall in this example means, out all truly diabetic people, how many of them did we correctly classify? (TP / TP + FN). The true positive rate is synonymous with recall. If there are patients who have diabetes in the test set, this logistic regression model can identify it 16% of the time. This is really quite low!\nHowever, remember that there is a trade off between precision and recall. The more precise we become, the lower the recall. The greater recall, the lower the precision.\n\n\nF1 score\nThe F1 score is the harmonic average between precision and recall. You optimise the F1 metric to get a healthy balance between precision and recall. Which could be a problem when trying to classify with an imbalanced dataset. For instance, where there is a minority class, for instance a small amount of observations that you would like to identify.\n\n\nROC curve\nSensitivity is the probability of detecting a TP from all positive observations (TP / TP + FN). In other words, a specificity of 80% will mean that we will identify 80% of people that are diabetic, but we will miss the other 20% This is also called the true positive rate and is also known as recall!\nSpecificity is the probability of correctly identifying those without the disease. This means we will be able to correctly identify individuals without the disease but will missclassify the others (TN / TN + FP). This is also called the true negative rate. Let’s have a look at an ROC curve for our data\n\ndef plotROC (X_test, y_test, model): \n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n\nplotROC(X_test, y_test, model)\n\n\n\n\n\n\n\n\nSo all in all the model works well with detecting those without diabetes, but the precision and recall for detecting individuals with diabetes, well, that could be very much improved! This is the challenge that we face when working with such unbalanced datasets which is typical with health-related datasets. Let’s investigate more on how other models deal with this problem and how we can fine tune the hyper parameters, for a model with more precision and a better capability to recall."
  },
  {
    "objectID": "posts/logistic-regression.html#the-dataset",
    "href": "posts/logistic-regression.html#the-dataset",
    "title": "Logistic Regression (classification)",
    "section": "The dataset",
    "text": "The dataset\nFor this dataset we are going to use the CDC Diabetes Health Indicators dataset which I came across on the UCI Irvine Machine Learning Repository. We are also going to program this in Python, due to the libraries we can utilise with it.\n\n# These are the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter"
  },
  {
    "objectID": "posts/logistic-regression.html#exploratory-data-analysis-eda",
    "href": "posts/logistic-regression.html#exploratory-data-analysis-eda",
    "title": "Logistic Regression (classification)",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n# Let's load in our data\ndata_file = \"../data/diabetes_binary_health_indicators_BRFSS2015.csv\"\ndf = pd.read_csv(data_file)\n\n\n# Create a function that will give us some useful starting statistics about our dataset\n\ndef eda(df):\n    head = df.head()\n    shape = df.shape\n    missing_values = df.isnull().sum()\n    return head, shape, missing_values\n\nhead, shape, missing_values = eda(df)\n\n\nhead\n# We can see our label, y named 'Diabetes_binary' is the first feature in our dataset \n\n\n\n\n\n\n\n\nDiabetes_binary\nHighBP\nHighChol\nCholCheck\nBMI\nSmoker\nStroke\nHeartDiseaseorAttack\nPhysActivity\nFruits\n...\nAnyHealthcare\nNoDocbcCost\nGenHlth\nMentHlth\nPhysHlth\nDiffWalk\nSex\nAge\nEducation\nIncome\n\n\n\n\n0\n0.0\n1.0\n1.0\n1.0\n40.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n5.0\n18.0\n15.0\n1.0\n0.0\n9.0\n4.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n25.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n3.0\n0.0\n0.0\n0.0\n0.0\n7.0\n6.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n28.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n5.0\n30.0\n30.0\n1.0\n0.0\n9.0\n4.0\n8.0\n\n\n3\n0.0\n1.0\n0.0\n1.0\n27.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n11.0\n3.0\n6.0\n\n\n4\n0.0\n1.0\n1.0\n1.0\n24.0\n0.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n0.0\n2.0\n3.0\n0.0\n0.0\n0.0\n11.0\n5.0\n4.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nshape\n# We have 253,680 individuals and 22 features in total\n\n(253680, 22)\n\n\n\nmissing_values\n# No missing values have been observed, which means no need to drop or impute any values \n\nDiabetes_binary         0\nHighBP                  0\nHighChol                0\nCholCheck               0\nBMI                     0\nSmoker                  0\nStroke                  0\nHeartDiseaseorAttack    0\nPhysActivity            0\nFruits                  0\nVeggies                 0\nHvyAlcoholConsump       0\nAnyHealthcare           0\nNoDocbcCost             0\nGenHlth                 0\nMentHlth                0\nPhysHlth                0\nDiffWalk                0\nSex                     0\nAge                     0\nEducation               0\nIncome                  0\ndtype: int64\n\n\n\n# Let's seperate the data into the feature (independent) variables and the target variable (label) \n\ndef prepData (df, \n              y_label_name):\n    df = pd.read_csv(data_file)\n    X = df.loc[:, df.columns != y_label_name]\n    y = df[y_label_name]\n    return df, X, y\n  \ndf, X, y = prepData(df, y_label_name = 'Diabetes_binary')\n \n\ny\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n253675    0.0\n253676    1.0\n253677    0.0\n253678    0.0\n253679    1.0\nName: Diabetes_binary, Length: 253680, dtype: float64"
  },
  {
    "objectID": "posts/logistic-regression.html#the-data",
    "href": "posts/logistic-regression.html#the-data",
    "title": "Logistic Regression (classification)",
    "section": "The data",
    "text": "The data\nFor this dataset we are going to use the CDC Diabetes Health Indicators dataset which I came across on the UCI Irvine Machine Learning Repository. We are also going to program this in Python, due to the libraries we can utilise with it.\n\n# These are the libraries which we will require:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom collections import Counter"
  }
]