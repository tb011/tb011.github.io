---
title: "Cross-validation"
editor: visual
categories:
  - Machine learning
  - Cross-validation
  - Data
---

The question:

When we do a 25% / 75% split. How do we know that the i.e. last quarter of the dataset (or a random selection of the dataset) is the best selection of the data to test on? It may randomly not be a good selection.

Bring in k-fold cross-validation.

## *K*-fold cross validation

Example: 4-fold cross validation (k = 4)

1\. split the data in *k*-(4) folds.

2\. Train on 3 partitions of the data, test on the remaining block. Take the below permutations

Train {2, 3, 4} ; Test {1}

\| 1 \| - test

\| 2 \| - train

\| 3 \| - train

\| 4 \| - train

Train {1, 3, 4} ; Test {2}

\| 1 \| - train

\| 2 \| - test

\| 3 \| - train

\| 4 \| - train

Train {1, 2, 4} ; Test {3}

\| 1 \| - train

\| 2 \| - train

\| 3 \| - test

\| 4 \| - train

Train {1, 2, 3} ; Test {4}

\| 1 \| - train

\| 2 \| - train

\| 3 \| - train

\| 4 \| - test

We keep track of how well the machine learning algorithm does on teach test and then then take the average of the the test data scores.

We can then use this metric to compare its performance amongst other algorithms.

*K* = 10, is a popular default. Meaning we train on 9 folds of the data, and test on the remaining unseen fold. In using *k*-fold cross-validation it means that every block of data is used for testing, rather than just 25%, for instance.

## Leave one out cross-validation

We could even take this example to the extreme and call each row (record/sample/ individual) as a "fold" and leave one individual out, and see how well the model is at classifying the remaining individual.

## Stratified *k*-fold cross-validation

This is where the dilemma of unbalanced datasets come to bite. Let me give an example. In our typical health data set up, within a population the likelihood of observing a with a disease (i.e. COPD) is lower than that of observing an individual without COPD. The problem is exacerbated further with rare diseases. Therefore *k*-fold can fail to work, because we would like each fold to contain individuals with the disease. However, just by random chance, or if a disease is not very prevalent in a population our our sample of the population, then we may risk the event of not having cases (individuals with the disease) in one or many folds of our training data, which as we can envisage would have a poor performance when our model comes across a case in the first time in the test data!

Therefore, we can use stratified k-fold cross validation to keep the same percentage of samples for each class, in each fold. For example, if our dataset has 10% of patients having COPD and 90% of patients not having COPD. Then by using [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) the sci-kit learn library to keep the proportion of cases/non-cases (10% / 90%) the same in each of our *k* folds.

## Repeated stratified *k*-fold cross-validation

I came across [a suggestion](https://machinelearningmastery.com/multinomial-logistic-regression-with-python/) of using a repeated stratified *k*-fold cross-validation when I was training a logisitic regression model on a [diabetes dataset](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset). I wondered if it could improve my models performance. And then everything else led from there....

When using repeated stratified *k*-fold cross-validation we can repeat the random sampling of individuals in with a different randomisation in each repetition using [RepeatedStratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html). This would mean that we would give our parameter *k,* for the number of folds and the parameter, *n* for the number of repeats. The result would be the average result across all folds from all runs. Please note that you are working on an balanced dataset, you can also use [repeated *k-*fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html) cross-validation without the stratification!

## Final comments

It can be much slower to partition the data in to *k*-folds (or repeated/stratified/repeated stratified *k*-folds) rather than splitting the data into a training and test set. Some times, depending on the application, computation time (in training and/or running the model) can be more important scoring a few decimal places higher on accuracy. So that is a trade off that something else for us to consider!
